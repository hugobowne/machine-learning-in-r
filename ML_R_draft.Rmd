---
title: "Introduction to Machine Learning in R"
output:
  html_document: default
  html_notebook: default
---

## Getting set up computationally

```{r message=FALSE, warning=FALSE}
# Run this cell to install & load the required packages
# If you need to install them, uncomment the lines of code below
#install.packages("tidyverse")
#install.packages("kernlab")
#install.packages("ddalpha")
#install.packages("caret")
#install.packages("GGally")
#install.packages("gmodels")

# Load packages
library(tidyverse)
library(kernlab)
library(ddalpha)
library(caret)
library(GGally)
library(gmodels)
```



## Loading your data

It's time to import the first dataset that we'll work with, the [Breast Cancer Wisconsin (Diagnostic) Data Set](http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29) from the UCI Machine Learning repository.

Do this and check out the first several rows:

```{r message=FALSE, warning=FALSE}
# Load data
df <- read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data",
               col_names = FALSE)
# Check out head of dataframe
df %>% head()
```

Note to HBA: get/create copy of dataset with meaningful variale. names 

**Discussion:** What are the variables in the dataset? Follow the link to UCI above to find out.

Before thinking about modeling, have a look at your data. There's no point in throwing a $10^4$ layer convolutional neural network (whatever that means) at your data before you even know what youre dealing with.

You'll first remove the column, which is the unique identifier of each row (explain why, Hugo):

```{r}
# Remove first column 
df <- df[2:31]
# View head
df %>% head()
```

**Question:** How many features are there and how many observations?

Now there are too many features to plot so you'll plot the first 5 in a pair-plot
Too many features to plot so lets plot the first 5:

```{r}
# Pair-plot of first 5 features
ggpairs(df[1:5], aes(colour=X2, alpha=0.4))
```


**Discussion:** What can you see here?

Note that the features have widely varying centers and scales (means and standard deviations) so we'll want center and scale them in some situations (motivate more; also discuss caret API of `preProcess` & `predict`):

```{r}
# Center & scale data
ppv <- preProcess(df, method = c("center", "scale"))
df_tr <- predict(ppv, df)
# Summarize first 5 columns
df_tr[1:5] %>% summary()
```


Now plot the centred & scaled features:

```{r}
# Pair-plot of transformed data
ggpairs(df_tr[1:5], aes(colour=X2))
```


**Discussion:** How does this compare to your previous pairplot?

## Unsupervised Learning I: dimensionality reduction

*Machine learning* is the science and art of giving computers the ability to learn to make decisions from data without being explicitly programmed.

*Unsupervised learning*, in essence, is the machine learning task of uncovering hidden patterns and structures from unlabeled data. For example, a business may wish to group its customers into distinct categories based on their purchasing behavior without knowing in advance what these categories maybe. This is known as clustering, one branch of unsupervised learning.

Another form of *unsupervised learning*, is _dimensionality reduction_: in the breat cancer dataset, for example, there are too many features to keep track of. What if we could reduced the number of features yet still keep much of the information? 

**Discussion:** Look at features X3 and X5. Do you think that we could reduce them to one feature and keep much of the information?


Principal component analysis  will extract the features with the largest variance. Here let's take the 1st 2 principal components and plot them, colored by tumour diagnosis.

Aside: *Supervised learning*, which we'll get to soone enough, is the branch of machine learning that involves predicting labels, such as whether a tumour will be *benign* or *malignant*.


```{r}
# PCA on data
ppv_pca <- preProcess(df, method = c("center", "scale", "pca"))
df_pc <- predict(ppv_pca, df)
# Plot 1st 2 principal components
ggplot(df_pc, aes(x = PC1, y = PC2, colour = X2)) + geom_point()
```

**Note:** Explain the basics of PCA here, HBA.

## Unsupervised Learning II: Clustering

You can try to cluster your data points using k-means (explain):

```{r}
# k-means
km.out <- kmeans(df[,2:10], centers=2, nstart=20)
summary(km.out)
km.out$cluster
```

Cross-tab (explain) to compare clustering with your known labels:


```{r}
# Cross-tab of clustering & known labels
CrossTable(df$X2, km.out$cluster)
```

**Discussion:** How well did the k-nearest neighbors do at clustering the tumor data?


## Supervised Learning

Try to predict diagnosis based on geometrical measurements.

**Discussion:** Look at your pair plot above. What would a baseline model there be?

**TO DO:** Build model that predicts diagnosis based on whether $X3 > 12$ or something similar.
```{r}
# Build baseline model
```

This is not a great model but it does give us a baseline: any model that we build later needs to perform better than this one.

Whoa: what do we mean by _model performance_ here? There are many _metrics_ to determine model performance and here we'll use _accuracy_, the percentage of the data that the model got correct.

Calculate baseline model accuracy:

```{r}
# Calculate accuracy

```

Now it's time to build an ever so slightly more complex model, a logistic regression.

### Logistic regression

Let's build a logistic regression. Note: we'll need to explain logistic regression (logreg) and why it's great, either here or below bulding the model. **Important:** logreg outputs a probability, which you'll then convert to a prediciton. Say more abuot this.

**Note on terminology:**

- The _target variable_ is the one you are trying to predict;
- Other variables are known as _features_ (or _predictor variables_).

We first need to change `df$X2`, the _target variable_, to a factor:

```{r}
# What is the class of X2?
class(df$X2)
# Change it to a factor
df$X2 <- as.factor(df$X2)
# What is the class of X2 now?
class(df$X2)
```

Now build that logreg model:

```{r}
# Build model
model <- glm(X2 ~ ., family = "binomial", df)
# Predict probability on the same dataset
p <- predict(model, df, type="response")
# Convert probability to prediction "M" or "B"
pred <- ifelse(p > 0.50, "M", "B")

# Create confusion matrix
confusionMatrix(as.factor(pred), df$X2)
```

**Discussion:** From the above, can you say what the model accuracy is? 

_BUT_ this is the accuracy on the data that you trained the model on

SO! train test split (say more about this: something like to see how well our model will generalize, we split the data in two, train on one part (training data), then predict on the other (testing data) and check out the accuracy on the testing data):

(note: mention overfitting/underfitting)

```{r}
# Set seed for reproducible results
set.seed(42)
# Train test split
inTraining <- createDataPartition(df$X2, p = .75, list=FALSE)
# Create train set
df_train <- df[ inTraining,]
# Create test set
df_test <- df[-inTraining,]
# Fit model to train set
model <- glm(X2 ~ ., family="binomial", df_train)
# Predict on test set
p <- predict(model, df_test, type="response")
pred <- ifelse(p > 0.50, "M", "B")

# Create confusion matrix
confusionMatrix(as.factor(pred), df_test$X2)
```

Now you'll build more complex models: decision trees.

### Random Forests

This caret API is so cool you can use it for lots of models. You'll build random forests below (introduce decision trees first, then describe random forest and hyperparameters).

But first, there's a pretty cool alternative to train test split called k-fold cross validation.


#### Cross Validation

To choose your random forest hyperparameter `max_depth`, for example, you'll use a variation on test train split called cross validation.

We begin by splitting the dataset into 5 groups or _folds_ (include image). Then we hold out the first fold as a test set, fit our model on the remaining four folds, predict on the test set and compute the metric of interest. Next we hold out the second fold as our test set, fit on the remaining data, predict on the test set and compute the metric of interest. Then similarly with the third, fourth and fifth.

As a result we get five values of accuracy, from which we can compute statistics of interest, such as the median and/or mean and 95% confidence intervals.

We do this for each value of each hyperparameter that we're tuning and choose the set of hyperparameters that performs the best. This is called _grid search_ if we specify the hyperparameter values we wish to try and called _random search_, which searched randomly through the hyperparameter space (see more [here](http://topepo.github.io/caret/random-hyperparameter-search.html)).

You'll first build a random forest with a grid containing 1 hyperparameter to get a feel for it.

```{r}
# Create model with default paramters
control <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "Accuracy"
mtry <- sqrt(ncol(df))
tunegrid <- expand.grid(.mtry=mtry)
rf_default <- train(X2~., data=df, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
print(rf_default)
```

Now try your hand at a random search:

```{r}
# Random Search
control <- trainControl(method="repeatedcv", number=5, repeats=3, search="random")
mtry <- sqrt(ncol(df))
rf_random <- train(X2~., data=df, method="rf", metric=metric, tuneLength=15, trControl=control)
print(rf_random)

```

And plot the results:

```{r}
plot(rf_random)
```

### What we can discuss adding

* Section on `glmnet`, regularized regression and variable selection
* Section on regression challenges, that is, predicting a continuously varying target variable instead of a category
* Section and/or exercises using genomic data.

